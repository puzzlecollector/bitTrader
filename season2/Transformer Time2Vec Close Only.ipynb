{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "from tensorflow.keras.models import *\n",
    "from tensorflow.keras.layers import *  \n",
    "from tensorflow.keras.callbacks import *\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import random\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow_probability import distributions as tfd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((10572180, 12), (919320, 12), (738300, 12), (535, 3))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## season 2 dataframes \n",
    "train_x_2 = pd.read_csv('train_x_df.csv')\n",
    "train_y_2 = pd.read_csv('train_y_df.csv') \n",
    "test_x_2 = pd.read_csv('test_x_df.csv') \n",
    "submission = pd.read_csv('sample_submission.csv') \n",
    "\n",
    "train_x_2.shape, train_y_2.shape, test_x_2.shape, submission.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((7661, 1380, 10), (7661, 120, 10), (535, 1380, 10))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def df2d_to_array3d(df_2d):\n",
    "    feature_size = df_2d.iloc[:,2:].shape[1]\n",
    "    time_size = len(df_2d.time.value_counts())\n",
    "    sample_size = len(df_2d.sample_id.value_counts())\n",
    "    array_3d = df_2d.iloc[:,2:].values.reshape([sample_size, time_size, feature_size])\n",
    "    return array_3d\n",
    "\n",
    "\n",
    "x_train = df2d_to_array3d(train_x_2) \n",
    "y_train = df2d_to_array3d(train_y_2) \n",
    "x_test = df2d_to_array3d(test_x_2) \n",
    "\n",
    "x_train.shape, y_train.shape, x_test.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_series(x_series, y_series, y_predicted):\n",
    "    #입력 series와 출력 series를 연속적으로 연결하여 시각적으로 보여주는 코드 입니다.\n",
    "    plt.plot(x_series, label = 'input_series')\n",
    "    plt.plot(np.arange(len(x_series), len(x_series)+len(y_series)),\n",
    "             y_series, label = 'actual_series') \n",
    "    plt.plot(np.arange(len(x_series), len(x_series)+len(y_predicted)),\n",
    "             y_predicted, label = 'predicted_series') \n",
    "    #plt.axhline(1, c = 'red')\n",
    "    plt.legend()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_predicted_series(x_series, y_predicted):\n",
    "    #입력 series와 출력 series를 연속적으로 연결하여 시각적으로 보여주는 코드 입니다.\n",
    "    plt.plot(x_series, label = 'input_series')\n",
    "    plt.plot(np.arange(len(x_series), len(x_series)+len(y_predicted)),\n",
    "             y_predicted, label = 'predicted_series') \n",
    "    #plt.axhline(1, c = 'red')\n",
    "    plt.legend()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7661, 1500)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train_close = x_train[:,:,4] \n",
    "y_train_close = y_train[:,:,4] \n",
    "x_test_close = x_test[:,:,4] \n",
    "\n",
    "close_prices = np.concatenate([x_train_close, y_train_close], axis = 1) \n",
    "close_prices.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7661/7661 [00:09<00:00, 836.60it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((10112520, 60), (10112520,))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "K = 120 \n",
    "N = 60 \n",
    "seq_len = 1500 \n",
    "features = 1\n",
    "X = [] \n",
    "Y = [] \n",
    "\n",
    "for j in tqdm(range(close_prices.shape[0]), position = 0, leave = True): \n",
    "    i = 0\n",
    "    while i+N+K < 1500: \n",
    "        X.append(close_prices[j, i:i+N]) \n",
    "        Y.append(close_prices[j, i+N+K]) # get close prices as target\n",
    "        i += 1   \n",
    "        \n",
    "        \n",
    "X = np.asarray(X) \n",
    "Y = np.asarray(Y)\n",
    "\n",
    "X.shape, Y.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10112520, 60, 1)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = X.reshape((-1,N,1))\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Time2Vector(Layer):\n",
    "    def __init__(self, seq_len, **kwargs):\n",
    "        super(Time2Vector, self).__init__()\n",
    "        self.seq_len = seq_len\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        '''Initialize weights and biases with shape (batch, seq_len)'''\n",
    "        self.weights_linear = self.add_weight(name='weight_linear',\n",
    "                                              shape=(int(self.seq_len),),\n",
    "                                              initializer='glorot_uniform',\n",
    "                                              trainable=True)\n",
    "    \n",
    "        self.bias_linear = self.add_weight(name='bias_linear',\n",
    "                                           shape=(int(self.seq_len),),\n",
    "                                           initializer='glorot_uniform',\n",
    "                                           trainable=True)\n",
    "    \n",
    "        self.weights_periodic = self.add_weight(name='weight_periodic',\n",
    "                                                shape=(int(self.seq_len),),\n",
    "                                                initializer='glorot_uniform',\n",
    "                                                trainable=True)\n",
    "\n",
    "        self.bias_periodic = self.add_weight(name='bias_periodic',\n",
    "                                             shape=(int(self.seq_len),),\n",
    "                                             initializer='glorot_uniform',\n",
    "                                             trainable=True)\n",
    "\n",
    "    def call(self, x):\n",
    "        '''Calculate linear and periodic time features'''\n",
    "        x = tf.math.reduce_mean(x[:,:,:], axis=-1) \n",
    "        time_linear = self.weights_linear * x + self.bias_linear # Linear time feature\n",
    "        time_linear = tf.expand_dims(time_linear, axis=-1) # Add dimension (batch, seq_len, 1)\n",
    "    \n",
    "        time_periodic = tf.math.sin(tf.multiply(x, self.weights_periodic) + self.bias_periodic)\n",
    "        time_periodic = tf.expand_dims(time_periodic, axis=-1) # Add dimension (batch, seq_len, 1)\n",
    "        return tf.concat([time_linear, time_periodic], axis=-1) # shape = (batch, seq_len, 2)\n",
    "   \n",
    "    def get_config(self): # Needed for saving and loading model with custom layer\n",
    "        config = super().get_config().copy()\n",
    "        config.update({'seq_len': self.seq_len})\n",
    "        return config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transformer_block(inputs, node, drop_rate, activation): \n",
    "    attn_output = MultiHeadAttention(num_heads = 2, key_dim = node)(inputs, inputs) \n",
    "    attn_output = Dropout(drop_rate)(attn_output) \n",
    "    out1 = LayerNormalization(epsilon=1e-6)(inputs + attn_output) \n",
    "    ffn_output = Dense(node, activation = activation)(out1) \n",
    "    ffn_output = Dense(node)(ffn_output) \n",
    "    ffn_output = Dropout(drop_rate)(ffn_output)\n",
    "    out2 = LayerNormalization(epsilon=1e-6)(out1 + ffn_output) \n",
    "    return out2\n",
    "    \n",
    "    \n",
    "def build_transformer(node = 64, activation = 'relu', drop_rate = 0.2, num_layers = 3): \n",
    "    time_embedding = Time2Vector(N)\n",
    "    inputs = Input((N, 1))\n",
    "    bn = BatchNormalization()(inputs) \n",
    "    x = time_embedding(bn) \n",
    "    x = Concatenate()([bn, x]) \n",
    "    x = Conv1D(node*2, 5, activation = activation)(x) \n",
    "    x = MaxPooling1D(3)(x) \n",
    "    x = Dropout(drop_rate)(x) \n",
    "    x = Conv1D(node, 5, activation = activation)(x) \n",
    "    x = MaxPooling1D(3)(x) \n",
    "    x = Dropout(drop_rate)(x) \n",
    "    \n",
    "    positions = tf.range(start=0, limit=x.shape[1], delta=1)\n",
    "    positions = Embedding(input_dim = x.shape[1], output_dim = node)(positions) \n",
    "    x = x + positions \n",
    "    \n",
    "    for i in range(num_layers): \n",
    "        x = transformer_block(x, node, drop_rate, activation)\n",
    "        \n",
    "    x = GlobalMaxPooling1D()(x)\n",
    "    x = Dropout(drop_rate)(x)\n",
    "    x = Dense(1, activation = 'relu')(x) \n",
    "    model = Model(inputs=inputs,outputs=x) \n",
    "    model.compile(loss = 'mape', optimizer = 'adam', metrics=['mae','mse','mape']) \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            [(None, 60, 1)]      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 60, 1)        4           input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "time2_vector_1 (Time2Vector)    (None, 60, 2)        240         batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 60, 3)        0           batch_normalization_1[0][0]      \n",
      "                                                                 time2_vector_1[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_2 (Conv1D)               (None, 56, 128)      2048        concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1D)  (None, 18, 128)      0           conv1d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_5 (Dropout)             (None, 18, 128)      0           max_pooling1d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_3 (Conv1D)               (None, 14, 64)       41024       dropout_5[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1D)  (None, 4, 64)        0           conv1d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_6 (Dropout)             (None, 4, 64)        0           max_pooling1d_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "tf.__operators__.add_3 (TFOpLam (None, 4, 64)        0           dropout_6[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "multi_head_attention_1 (MultiHe (None, 4, 64)        33216       tf.__operators__.add_3[0][0]     \n",
      "                                                                 tf.__operators__.add_3[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_7 (Dropout)             (None, 4, 64)        0           multi_head_attention_1[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "tf.__operators__.add_4 (TFOpLam (None, 4, 64)        0           tf.__operators__.add_3[0][0]     \n",
      "                                                                 dropout_7[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_2 (LayerNor (None, 4, 64)        128         tf.__operators__.add_4[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 4, 64)        4160        layer_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 4, 64)        4160        dense_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_8 (Dropout)             (None, 4, 64)        0           dense_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "tf.__operators__.add_5 (TFOpLam (None, 4, 64)        0           layer_normalization_2[0][0]      \n",
      "                                                                 dropout_8[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_3 (LayerNor (None, 4, 64)        128         tf.__operators__.add_5[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "multi_head_attention_2 (MultiHe (None, 4, 64)        33216       layer_normalization_3[0][0]      \n",
      "                                                                 layer_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dropout_9 (Dropout)             (None, 4, 64)        0           multi_head_attention_2[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "tf.__operators__.add_6 (TFOpLam (None, 4, 64)        0           layer_normalization_3[0][0]      \n",
      "                                                                 dropout_9[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_4 (LayerNor (None, 4, 64)        128         tf.__operators__.add_6[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 4, 64)        4160        layer_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dense_6 (Dense)                 (None, 4, 64)        4160        dense_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_10 (Dropout)            (None, 4, 64)        0           dense_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "tf.__operators__.add_7 (TFOpLam (None, 4, 64)        0           layer_normalization_4[0][0]      \n",
      "                                                                 dropout_10[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_5 (LayerNor (None, 4, 64)        128         tf.__operators__.add_7[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "multi_head_attention_3 (MultiHe (None, 4, 64)        33216       layer_normalization_5[0][0]      \n",
      "                                                                 layer_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dropout_11 (Dropout)            (None, 4, 64)        0           multi_head_attention_3[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "tf.__operators__.add_8 (TFOpLam (None, 4, 64)        0           layer_normalization_5[0][0]      \n",
      "                                                                 dropout_11[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_6 (LayerNor (None, 4, 64)        128         tf.__operators__.add_8[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dense_7 (Dense)                 (None, 4, 64)        4160        layer_normalization_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dense_8 (Dense)                 (None, 4, 64)        4160        dense_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_12 (Dropout)            (None, 4, 64)        0           dense_8[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "tf.__operators__.add_9 (TFOpLam (None, 4, 64)        0           layer_normalization_6[0][0]      \n",
      "                                                                 dropout_12[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_7 (LayerNor (None, 4, 64)        128         tf.__operators__.add_9[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_1 (GlobalM (None, 64)           0           layer_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dropout_13 (Dropout)            (None, 64)           0           global_max_pooling1d_1[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dense_9 (Dense)                 (None, 1)            65          dropout_13[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 168,757\n",
      "Trainable params: 168,755\n",
      "Non-trainable params: 2\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = build_transformer() \n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "71104/71104 [==============================] - 874s 12ms/step - loss: 2.2092 - mae: 0.0221 - mse: 0.0036 - mape: 2.2092 - val_loss: 1.0007 - val_mae: 0.0101 - val_mse: 4.1430e-04 - val_mape: 1.0007\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.00072, saving model to Transformer_epoch_001_val_1.001.h5\n",
      "Epoch 2/20\n",
      "71104/71104 [==============================] - 853s 12ms/step - loss: 1.0774 - mae: 0.0108 - mse: 3.0664e-04 - mape: 1.0774 - val_loss: 1.0773 - val_mae: 0.0109 - val_mse: 4.5265e-04 - val_mape: 1.0773\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 1.00072\n",
      "Epoch 3/20\n",
      "71104/71104 [==============================] - 853s 12ms/step - loss: 1.0714 - mae: 0.0107 - mse: 3.0151e-04 - mape: 1.0714 - val_loss: 1.0443 - val_mae: 0.0105 - val_mse: 4.2726e-04 - val_mape: 1.0443\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1.00072\n",
      "Epoch 4/20\n",
      "71104/71104 [==============================] - 857s 12ms/step - loss: 1.0724 - mae: 0.0107 - mse: 3.0197e-04 - mape: 1.0724 - val_loss: 1.0233 - val_mae: 0.0102 - val_mse: 4.3217e-04 - val_mape: 1.0233\n",
      "\n",
      "Epoch 00004: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1.00072\n",
      "Epoch 5/20\n",
      "71104/71104 [==============================] - 850s 12ms/step - loss: 1.0610 - mae: 0.0106 - mse: 2.9887e-04 - mape: 1.0610 - val_loss: 0.9962 - val_mae: 0.0100 - val_mse: 3.9461e-04 - val_mape: 0.9962\n",
      "\n",
      "Epoch 00005: val_loss improved from 1.00072 to 0.99622, saving model to Transformer_epoch_005_val_0.996.h5\n",
      "Epoch 6/20\n",
      "21897/71104 [========>.....................] - ETA: 9:23 - loss: 1.0558 - mae: 0.0106 - mse: 2.9211e-04 - mape: 1.0558"
     ]
    }
   ],
   "source": [
    "model_path = 'Transformer_epoch_{epoch:03d}_val_{val_loss:.3f}.h5'\n",
    "learning_rate_reduction = ReduceLROnPlateau(monitor = 'val_loss', patience = 3, verbose = 1, factor = 0.5)\n",
    "checkpoint = ModelCheckpoint(filepath = model_path, monitor = 'val_loss', verbose = 1, save_best_only = True)\n",
    "early_stopping = EarlyStopping(monitor = 'val_loss', patience = 10) \n",
    "\n",
    "\n",
    "history = model.fit(X, \n",
    "                    Y, \n",
    "                    batch_size = 128, \n",
    "                    epochs = 20, \n",
    "                    callbacks = [learning_rate_reduction, checkpoint], \n",
    "                    validation_split = 0.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = load_model('')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = [] \n",
    "for j in tqdm(range(x_test.shape[0]), position = 0, leave = True): \n",
    "    for i in range(seq_len-K-N-120, seq_len-K-N):\n",
    "        X_test.append(x_test_close[j, i:i+N])  \n",
    "\n",
    "X_test = np.asarray(X_test).reshape((-1,N,1))\n",
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted = best_model.predict(X_test) \n",
    "predicted = predicted.reshape((-1,120)) \n",
    "predicted.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Adjust predicted prices to be connected with the buy price \n",
    "for i in tqdm(range(predicted.shape[0]), position = 0, leave = True):\n",
    "    buy_price = x_test_close[i,-1] \n",
    "    if predicted[i,0] > buy_price:\n",
    "        diff = predicted[i,0] - buy_price \n",
    "        predicted[i,:] -= diff \n",
    "    elif predicted[i,0] < buy_price:  \n",
    "        diff = buy_price - predicted[i,0] \n",
    "        predicted[i,:] += diff\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## create submission dataframe version 1 \n",
    "buy_quantities = [] \n",
    "sell_times = [] \n",
    "\n",
    "for i in tqdm(range(predicted.shape[0]), position = 0, leave = True): \n",
    "    sell_time = np.argmax(predicted[i,:]) \n",
    "    sell_times.append(sell_time) \n",
    "    buy_price = x_test_close[i,-1] \n",
    "    cnt = 0 \n",
    "    for j in range(120): \n",
    "        if predicted[i,j] >= buy_price:    \n",
    "            cnt += 1 \n",
    "    buy_quantity = cnt/120 \n",
    "    buy_quantities.append(buy_quantity)\n",
    "    \n",
    "submission.iloc[:,1] = buy_quantities \n",
    "submission.iloc[:,2] = sell_times  \n",
    "submission.to_csv('transformers_strong_mape.csv',index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_train = best_model.predict(X[:2400,])\n",
    "pred_train = pred_train.reshape((-1,120))\n",
    "pred_train.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in tqdm(range(pred_train.shape[0]), position = 0, leave = True): \n",
    "    buy_price = x_train_close[i,-1] \n",
    "    if pred_train[i,0] > buy_price:\n",
    "        diff = pred_train[i,0] - buy_price \n",
    "        pred_train[i,:] -= diff \n",
    "    elif pred_train[i,0] < buy_price:  \n",
    "        diff = buy_price - pred_train[i,0] \n",
    "        pred_train[i,:] += diff\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(20): \n",
    "    plt.plot() \n",
    "    plot_series(x_train_close[i,:], y_train_close[i,:], pred_avg[i,:])\n",
    "    plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
